<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description"
          content="OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe.">
    <meta property="og:title"
          content="OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe"/>
    <meta property="og:description"
          content="OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe."/>
    <meta property="og:url" content="https://github.com/EvolvingLMMs-Lab/OpenMMReasoner/"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/images/uniir_teaser.jpg"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>

    <title>OpenMMReasoner</title>
    <link rel="icon" type="image/x-icon" href="">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="static/js/jquery.min.js"></script>
    <script src="static/js/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <link rel="stylesheet" type="text/css" href="static/css/jquery.dataTables.css">
    <script type="text/javascript" charset="utf8" src="static/js/jquery-3.5.1.js"></script>
    <script type="text/javascript" charset="utf8" src="static/js/jquery.dataTables.js"></script>
</head>

<body>

<!-- <p></p>
<br> <br><br><br> -->

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- <h1 class="title is-1 publication-title"> -->
                    <h1 class="title is-1 vj-hero-title-cyber">
                        OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe
                    </h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                            <a href="https://kcz358.github.io/" target="_blank">Kaichen Zhang</a><sup>*,1,2,4</sup>,
                         </span>
                        <span class="author-block">
                            <a href="https://kemingwu.github.io/" target="_blank">Keming Wu</a><sup>*,1,3,4</sup>,
                        </span>
                        <span class="author-block">
                                    <a href="https://mwxely.github.io/" target="_blank">Zuhao Yang</a><sup>1,2,4</sup>,
                        </span>
                        <span class="author-block">
                                <a href="https://brianboli.com/"
                                                  target="_blank">Bo Li</a><sup>2,4</sup>,
                        </span>
                        <span class="author-block">
                            <a href="https://kairuihu.github.io/"
                            target="_blank">Kairui Hu</a><sup>2,4</sup>,
                        </span>
                        <span class="author-block">
                            <a href="https://binwangthss.github.io/"
                                                  target="_blank">Bin Wang</a><sup>3</sup>,
                        </span>
                        <span class="author-block">
                            <a href="https://liuziwei7.github.io/"
                                                  target="_blank">Ziwei Liu</a><sup>2,4</sup>,
                        </span>
                        <span class="author-block">
                            <a href="https://xingxuanli.github.io/"
                            target="_blank">Xingxuan Li</a><sup>1,â€ </sup>,
                        </span>
                        <span class="author-block">
                                  <a href="https://lidongbing.github.io/" target="_blank">Lidong Bing</a><sup>1</sup>
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                            <span class="author-block">
                            <sup>1</sup>MiroMind AI,
                            <sup>2</sup>Nanyang Technological University,
                            <sup>3</sup>Tsinghua Univerisity,
                            <sup>4</sup>LMMs-Lab Team,
                            </span>
                            <br>
                            <span class="author-block"><sup>*</sup> Equal Contribution</span>, <sup>â€ </sup> Corresponding Author</span>
                            <br>
                            <span class="author-block">Corresponding to:</span>
                            <span class="author-block"><a href="mailto:xingxuan.li@miromind.ai">xingxuan.li@miromind.ai</a></span>
                            <!-- <span class="author-block"><a href="mailto:lidong.bing@miromind.ai">lidong.bing@miromind.ai</a></span> -->

                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2511.16334" target="_blank"
                                       class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                          <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>

                                <!-- Github link -->
                                <span class="link-block">
                                      <a href="https://github.com/EvolvingLMMs-Lab/OpenMMReasoner" target="_blank"
                                         class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        <i class="fab fa-github"></i>
                                      </span>
                                          <span>Code</span>
                                      </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://huggingface.co/datasets/OpenMMReasoner/OpenMMReasoner-SFT-874K" target="_blank"
                                       class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i>
                                        </span>
                                        <span>SFT-Data</span>
                                    </a>
                                  </span>
                                
                                  <span class="link-block">
                                    <a href="https://huggingface.co/datasets/OpenMMReasoner/OpenMMReasoner-RL-74K" target="_blank"
                                       class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i>
                                        </span>
                                        <span>RL-Data</span>
                                    </a>
                                  </span>

                                  <span class="link-block">
                                    <a href="https://huggingface.co/OpenMMReasoner/OpenMMReasoner-RL" target="_blank"
                                       class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                          ðŸ¤—
                                        </span>
                                        <span>OpenMMReasoner</span>
                                    </a>
                                  </span>
                            <span class="link-block">
                                    <a href="https://huggingface.co/papers/2511.16334" target="_blank" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon icon-emoji">ðŸš€</span>
                                        <span>Daily Paper</span>
                                    </a>
                                </span>

    <!-- <p></p> -->
     <br> <br> <br>

             <!-- Image carousel -->
    
     <section class="section hero is-light">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h1 class="title is-3">OpenMMReasoner</h1>
              <div class="content has-text-justified">
                  <p>Our contributions are threefold:</p>
      
                  <p><b>(1) High-quality multimodal reasoning data curation.</b>  
                  We provide the first systematic study on constructing SFT and RL datasets for multimodal reasoning, showing that both source diversity and answer diversity are crucial for building reliable supervision signals.</p>
      
                  <p><b>(2) A strong and reproducible SFT recipe.</b>  
                  We introduce a robust SFT pipeline with step-by-step validation, careful teacher-model selection, and cross-domain data integration, enabling the construction of a high-quality cold-start reasoning dataset.</p>
      
                  <p><b>(3) An advanced RL training recipe.</b>  
                  Through an extensive comparison of GSPO, GRPO, and DAPO, we identify the most stable and scalable RL strategy and build a reliable RL pipeline that significantly strengthens multimodal reasoning performance.</p>
              </div>
            </div>
          </div>
        </div>
      </section>
      

      <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/teaser.png" alt="OpenMMReasoner-Teaser" />
                            <h2 class="subtitle">
                                <b>Performance Comparison with State-of-the-Art Large Multimodal Reasoning Models across Various Benchmarks.</b>
                                Our proposed <b>OpenMMReasoner</b> consistently outperforms competing methods, highlighting its effectiveness in complex reasoning tasks.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section hero is-light">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h1 class="title is-3">OpenMMReasoner-Data</h1>
              <div class="content has-text-justified">
                <p>
                  <b>OpenMMReasoner-Data</b>, We propose two training recipes covering both the SFT and RL phases. The pipeline begins by collecting diverse data sources and selecting teacher models to generate new answer traces. During the RL phase, we explore different algorithm choices and filtering strategies, leading to our final optimized recipe.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
              <!-- Image carousel -->
      <section class="hero">
          <div class="hero-body">
              <div class="container is-max-desktop">
                  <div class="columns is-centered">
                      <div class="column is-full">
                          <div class="item">
                              <!-- Your image here -->
                              <img src="static/images/pipeline.png" alt="pipeline" />
                              <img src="static/images/data_distribution.png" alt="data_distribution" />
                              <!-- <h2 class="subtitle">
                                  Our dataset has the most diverse, highest-quality image editing pairs of any resolution.
                              </h2> -->
                          </div>
                      </div>
                  </div>
              </div>
          </div>
      </section>
      
    <section class="section hero is-light">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Experimental Results on Visual Reasoning Benchmarks</h2>
              <div class="content has-text-justified">
                <p>
                    We evaluate our approach on a suite of public visual reasoning benchmarks. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a <b>11.6%</b> improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. 
                </p>
              </div>
            </div>
          </div>
        </div>
    </section>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <img src="static/images/main_exp.png" alt="RL results" />
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section hero is-light">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Analysis and Insights for SFT</h2>
              <div class="content has-text-justified">
                <p>
                    <p>Our Analysis and Insights for SFT are as follows:</p>
      
                    <p><b>(1) Answer diversity enhances reasoning.</b>  
                        Increasing the diversity of generated answers consistently improves the model's overall reasoning performance, even when using the same question sources, suggesting that exposure to varied solutions strengthens understanding.</p>
        
                    <p><b>(2) Teacher model selection is crucial.</b>  
                        Distilling from a strong teacher model substantially boosts the model's reasoning ability while maintaining high data efficiency. Careful selection for teacher model directly affects the quality of the distilled dataset and the final model performance.</p>
        
                    <p><b>(3) Over-filtering reduces diversity and performance.</b>  
                        The best results are achieved without excessive filtering, indicating that maintaining greater answer diversity encourages more robust reasoning abilities.</p>

                    <p><b>(4) Cross-domain knowledge improves generalization.</b>  
                        Incorporating diverse data from multiple domains consistently enhances the model's overall reasoning capabilities across tasks.</p>
                </p>
              </div>
            </div>
          </div>
        </div>
    </section>


    <section class="section hero is-light">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Analysis and Insights for RL</h2>
              <div class="content has-text-justified">
                <p>
                    <p>Our Analysis and Insights for RL are as follows:</p>
      
                    <p><b>(1) GSPO outperforms other algorithms.</b>  
                        GSPO demonstrates superior stability and faster convergence compared to alternative methods in multimodal RL training.</p>
        
                    <p><b>(2) Token efficiency is crucial.</b>  
                        While increasing reasoning steps at test time can improve performance, excessive tokens reduce efficiency. Our results show that a smaller reasoning budget can achieve comparable or even better accuracy.</p>
        
                    <p><b>(3) Reasoning ability transfers across domains.</b>  
                        Gains in reasoning during training consistently translate into stronger performance across multiple domains.</p>
                </p>
              </div>
            </div>
          </div>
        </div>
    </section>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/RL_exp.png" alt="pipeline" />
                            <img src="static/images/RL_curve.png" alt="data_distribution" />
                            <img src="static/images/val_curves.png" alt="data_distribution" />
                            <!-- <h2 class="subtitle">
                                Our dataset has the most diverse, highest-quality image editing pairs of any resolution.
                            </h2> -->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- End image carousel -->

            <!-- Image carousel -->


                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


    <!-- BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Citation</h2>
            If you find our paper useful, please cite us with:
            <br><br>
            <pre><code>
@misc{zhang2025openmmreasonerpushingfrontiersmultimodal,
    title={OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe}, 
    author={Kaichen Zhang and Keming Wu and Zuhao Yang and Bo Li and Kairui Hu and Bin Wang and Ziwei Liu and Xingxuan Li and Lidong Bing},
    year={2025},
    eprint={2511.16334},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2511.16334}, 
}
</code></pre>
        </div>
    </section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic
                        Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                                                                                target="_blank">Nerfies</a> project
                        page. You are free to borrow the of this website, we just ask that you link back to this page in
                        the footer. <br> This website is licensed under a <a rel="license"
                                                                             href="http://creativecommons.org/licenses/by-sa/4.0/"
                                                                             target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

</body>
<style>
    .buttonGroup {
        text-align: center;
    }

    .buttonGroup > button {
        padding: 15px;
        color: white;
        background-color: #363636;
        border-radius: 5px;
    }

    .buttonGroup > button:hover {
        box-shadow: 5px;
    }
</style>

</html>
